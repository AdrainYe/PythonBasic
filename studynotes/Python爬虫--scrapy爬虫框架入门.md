# Python爬虫--scrapy爬虫框架入门
## 写在前面
之前写爬虫从来没用到过爬虫框架，其实之前写的都是学习爬虫写的小练手的demo，只能说是去网上抓取一点数据，直接一个python脚本解决完成爬虫所有的功能，利用for循环去抓取更多页面的数据。
后面学习到了代理池系统，稍微了解了一下爬虫其实是可以当做一个项目来看待，项目又是分模块的，不同的模块负责不同的功能，有的模块负责封装http请求，
有的模块负责处理请求（数据的抓取），有的模块负责数据的解析，有的模块负责数据的存储，这样一个框架的雏形就显现出来的，能够帮助我们更加专注和高效地进行数据的爬取。
对于scrapy，最开始并不想解释太多scrapy框架的各部分组成以及作用，直接在实际操作中去感受这个框架是如何分工合作的，每个模块负责什么样的功能，最后再去关注这个框架
的一些原理，应该是scrapy框架学习最好的方式了。
## scrapy爬虫的基本步骤
1. 创建一个scrapy爬虫项目。
2. 创建一个Spider爬虫类抓取网页内容并解析。
3. 定义数据模型（Item），将抓取到的数据封装到Item中。
4. 利用Item Pipeline存储抓取Item,即数据实体对象。
## 一、创建一个scrapy爬虫项目
安装好scrapy之后，运行命令**scrapy startproject 项目名**，就可以创建一个scrapy爬虫项目。
比如，运行 scrapy startproject test1,会生成一个test1的文件夹，目录结构如图：

![](https://github.com/daacheng/PythonBasic/blob/master/pic/scrapy1.png)
## 二、创建一个Spider爬虫类，抓取网页内容并解析。
